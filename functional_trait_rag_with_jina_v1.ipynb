{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNrTPQOBGtcczVQgdT9tmbR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smthomas1704/restoration-rag/blob/main/functional_trait_rag_with_jina_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download\n",
        "Download the zipped literature file from this Google Drive location"
      ],
      "metadata": {
        "id": "-PnazWDSRIPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0MbHH-sKD9i"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/smthomas1704/restoration-rag.git\n",
        "\n",
        "!pip install -r restoration-rag/requirements.txt\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4\n",
        "!pip install gdown==v4.6.3\n",
        "!pip install openai\n",
        "!pip install langchain_experimental\n",
        "\n",
        "!gdown https://drive.google.com/file/d/10_inKhFuY5O8Sel88ZvlTqODsbbh4ula/view?usp=drive_link -O /content/functional_trait_literature_zipped.zip --fuzzy\n",
        "\n",
        "!unzip /content/functional_trait_literature_zipped.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk and store\n",
        "\n",
        "In this portion, we'll chunk all the files into smaller paragraphs and use that for generating embeddings. We'll separately store the chunks so we can use it later, without having to re-download all the literature again.\n",
        "\n",
        "### References/Notes related to chunking\n",
        "1. https://openai.com/blog/new-and-improved-embedding-model\n",
        "2. text-embedding-ada-002 is the best model for text embedding generation\n",
        "3. https://www.pinecone.io/learn/chunking-strategies/\n",
        "4. https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pypdf\n",
        "\n",
        "### Possible strategies for chunking\n",
        "1. LaTex: LaTeX is a document preparation system and markup language often used for academic papers and technical documents. By parsing the LaTeX commands and environments, you can create chunks that respect the logical organization of the content (e.g., sections, subsections, and equations), leading to more accurate and contextually relevant results.\n",
        "2. Latex taxes a string as input, so we will need to read\n",
        "3. Most of these academic papers are written in Latex and then converted to PDF. Our best bet would be to convert the PDF to Latex format first and then use the Latex based chunker to chunk things. This way paragraphs and related information will be together and contextualized.\n",
        "4. On the other hand its not a guarantee that the document was first written in LaTex."
      ],
      "metadata": {
        "id": "aahoQgRnKfZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.docstore.document import Document\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "REPO_ID = \"collaborativeearth/functional_trait_papers\"\n",
        "FILENAME = \"function_trait_paper_small_chunks.jsonl\"\n",
        "\n",
        "# Download the chunks from Huggingface. We generated the chunks and uploaded it to Huggingface\n",
        "file_name = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\", local_dir=\"/content/\")\n",
        "\n",
        "print(file_name)\n",
        "chunks=[]\n",
        "\n",
        "with open(file_name, \"r\") as final:\n",
        "  chunks = json.load(final)\n",
        "\n",
        "prod_splits=[]\n",
        "\n",
        "for chunk in chunks:\n",
        "  prod_splits.append(Document(\n",
        "      page_content=chunk[\"page_content\"],\n",
        "      metadata={\n",
        "          \"source\": chunk[\"title\"],\n",
        "          \"id\": chunk[\"id\"]\n",
        "      }\n",
        "  ))\n",
        "\n",
        "print(prod_splits[0])\n",
        "print(prod_splits[1])"
      ],
      "metadata": {
        "id": "K5AJyyJUQo_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we will generate embeddings using different models for comparison\n",
        "First one is jina-embedding-l-en-v1"
      ],
      "metadata": {
        "id": "hKPYs7QuoLHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/vectorstore\"\n",
        "DB_JINA_EMBEDDING_PATH = 'vectorstore/db_jina-embedding-l-en-v1'\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name='jinaai/jina-embedding-b-en-v1',\n",
        "                                       model_kwargs={'device': 'cuda'})\n",
        "\n",
        "db = FAISS.from_documents(prod_splits, embeddings)\n",
        "serialized_bytes = db.serialize_to_bytes()\n",
        "with open(\"/content/vectorstore/serialized_db.txt\", \"wb\") as binary_file:\n",
        "    # Write bytes to file\n",
        "    binary_file.write(serialized_bytes)\n",
        "\n",
        "db.save_local(DB_JINA_EMBEDDING_PATH)"
      ],
      "metadata": {
        "id": "s2LvMgQOPF4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4a203e-eeb7-4f99-d04d-4f50097cee66"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/vectorstore’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = await db.asimilarity_search(\"How can plant functional traits be useful when designing a restoration project?\")\n",
        "print(len(docs))\n",
        "print(docs[0].metadata)\n",
        "print(docs[1].metadata)\n",
        "print(docs[2].metadata)\n",
        "print(docs[3].metadata)"
      ],
      "metadata": {
        "id": "q-hu1Nn3obvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4776c0d7-d067-485d-87f8-822529a2ab28"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "{'source': 'Functional traits and ecosystem services in ecological restoration', 'id': '5.3.8'}\n",
            "{'source': 'Restoring Ecosystem Services Tool (REST): a program for selecting species for restoration projects using a functional-trait approach', 'id': '10.4.1'}\n",
            "{'source': 'A Web-Based Software Platform for Restoration-Oriented Species Selection Based on Plant Functional Traits', 'id': '7.9.0'}\n",
            "{'source': 'Using soil amendments and plant functional traits to select native tropical dry forest species for the restoration of degraded Vertisols', 'id': '0.30.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "import tiktoken\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('COLABORATIVE_EARTH_KEY')\n",
        "llm_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "  memory_key='chat_history',\n",
        "  return_messages=False\n",
        ")\n",
        "retriever = db.as_retriever(\n",
        "    search_kwargs={\"k\": 20}\n",
        ")\n",
        "\n",
        "custom_template = \"\"\"You are an AI assistant for assisted restoration papers.\n",
        "You are given the following extracted parts of a long document.\n",
        "=========\n",
        "{context}\n",
        "=========\n",
        "Provide a conversational answer for the following question\n",
        "Question: {question}.\n",
        "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
        "Answer in Markdown:\"\"\"\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    template=custom_template,\n",
        "    input_variables=[\"context\", \"question\", \"source\"],\n",
        ")\n",
        "\n",
        "\n",
        "conversation_chain_with_reference_prompt = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        # chain_type=\"stuff\" will go through everything.\n",
        "        # chain_type=\"refine\",\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        verbose=True,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": custom_prompt}\n",
        ")\n",
        "\n",
        "\n",
        "conversation_chain_without_reference = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        # chain_type=\"stuff\" will go through everything.\n",
        "        chain_type=\"refine\",\n",
        "        # chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        # return_source_documents=True,\n",
        "        verbose=True,\n",
        "        memory=memory,\n",
        "        # combine_docs_chain_kwargs={\"question_prompt\": custom_prompt, \"refine_prompt\": custom_prompt}\n",
        ")\n",
        "\n",
        "query = \"I am designing a tropical dry forest restoration in an open field with no remaining tree cover. Should I plant species with higher or lower wood density to maximum initial survival?\"\n",
        "result = conversation_chain_with_reference_prompt({\"question\": query})\n",
        "# result = conversation_chain_without_reference({\"question\": query})\n",
        "answer = result[\"answer\"]\n",
        "\n",
        "answer"
      ],
      "metadata": {
        "id": "3K8kg2VYy3DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TODO\n",
        "1. Add a cross encoder after the retrieval stage to re-rank the results before feeding it to the API.\n",
        "2. Alternately, or maybe along with this, we may also want to combine several chunks to send as context, depending on the context lenght.\n",
        "3. Or perhaps we also chunk in small portions and combine results to send to OpenAI. This way answers can be formed based on chunks from different papers and different sections.\n",
        "4. Update this RAG to cite sources from the context provided. Refence: https://blog.langchain.dev/langchain-chat/\n",
        "\n",
        "https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a"
      ],
      "metadata": {
        "id": "OLk6TAvQyMip"
      }
    }
  ]
}