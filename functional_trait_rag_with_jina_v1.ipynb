{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMpz/8/1jD571ccF6scJSP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smthomas1704/restoration-rag/blob/main/functional_trait_rag_with_jina_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download\n",
        "Download the zipped literature file from this Google Drive location"
      ],
      "metadata": {
        "id": "-PnazWDSRIPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0MbHH-sKD9i"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/smthomas1704/restoration-rag.git\n",
        "\n",
        "!pip install -r restoration-rag/requirements.txt\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4\n",
        "!pip install gdown==v4.6.3\n",
        "!pip install openai\n",
        "!pip install langchain_experimental\n",
        "\n",
        "!gdown https://drive.google.com/file/d/10_inKhFuY5O8Sel88ZvlTqODsbbh4ula/view?usp=drive_link -O /content/functional_trait_literature_zipped.zip --fuzzy\n",
        "\n",
        "!unzip /content/functional_trait_literature_zipped.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk and store\n",
        "\n",
        "In this portion, we'll chunk all the files into smaller paragraphs and use that for generating embeddings. We'll separately store the chunks so we can use it later, without having to re-download all the literature again.\n",
        "\n",
        "### References/Notes related to chunking\n",
        "1. https://openai.com/blog/new-and-improved-embedding-model\n",
        "2. text-embedding-ada-002 is the best model for text embedding generation\n",
        "3. https://www.pinecone.io/learn/chunking-strategies/\n",
        "4. https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pypdf\n",
        "\n",
        "### Possible strategies for chunking\n",
        "1. LaTex: LaTeX is a document preparation system and markup language often used for academic papers and technical documents. By parsing the LaTeX commands and environments, you can create chunks that respect the logical organization of the content (e.g., sections, subsections, and equations), leading to more accurate and contextually relevant results.\n",
        "2. Latex taxes a string as input, so we will need to read\n",
        "3. Most of these academic papers are written in Latex and then converted to PDF. Our best bet would be to convert the PDF to Latex format first and then use the Latex based chunker to chunk things. This way paragraphs and related information will be together and contextualized.\n",
        "4. On the other hand its not a guarantee that the document was first written in LaTex."
      ],
      "metadata": {
        "id": "aahoQgRnKfZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.docstore.document import Document\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "REPO_ID = \"collaborativeearth/functional_trait_papers\"\n",
        "FILENAME = \"function_trait_paper_small_chunks.jsonl\"\n",
        "\n",
        "# Download the chunks from Huggingface. We generated the chunks and uploaded it to Huggingface\n",
        "file_name = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\", local_dir=\"/content/\")\n",
        "\n",
        "print(file_name)\n",
        "chunks=[]\n",
        "\n",
        "with open(file_name, \"r\") as final:\n",
        "  chunks = json.load(final)\n",
        "\n",
        "prod_splits=[]\n",
        "\n",
        "for chunk in chunks:\n",
        "  prod_splits.append(Document(\n",
        "      page_content=chunk[\"page_content\"],\n",
        "      metadata={\n",
        "          \"source\": chunk[\"title\"],\n",
        "          \"id\": chunk[\"id\"]\n",
        "      }\n",
        "  ))\n",
        "\n",
        "print(prod_splits[0])\n",
        "print(prod_splits[1])"
      ],
      "metadata": {
        "id": "K5AJyyJUQo_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e03e54-588d-4fe8-fff7-16b52488ee2a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/function_trait_paper_small_chunks.jsonl\n",
            "page_content='Tropical landscapes have been extensively degraded and deforested, but large-scale passive and active restoration projects have catalysed secondary forest regeneration over the last few decades (Chazdon, 2014).Tropical dry forests (TDFs) have a strong dry season of at least 3-4 months where little to no rain falls (Murphy & Lugo, 1986), which distinguishes TDFs from tropical wet forests, and offers a unique hurdle to restoration projects.Globally, 97% of TDFs are threatened by anthropogenic processes (Miles et al., 2006), and in Central America it is estimated that only 1.7% of the original extent of TDF exists (Griscom & Ashton, 2011;Miles et al., 2006).Despite the fact that TDFs are critically endangered (Janzen, 2002), the restoration of TDFs has been studied minimally compared to wetter tropical forests (Meli, 2003).Following agricultural abandonment in Central America in the 1990s, large tracts of land became available, and active and passive restoration techniques such as plantation establishment, enrichment planting and fire exclusion have been effective in the reestablishment of TDF (Griscom & Ashton, 2011).In north-west (NW) Costa Rica, the conservation of over 160,000 hectares of land in the Área de Conservación Guanacaste created one of the most effective TDF restoration projects in the world (Janzen, 2002).In the Área de Conservación Guanacaste, passive restoration techniques, including removal of unnatural fires and grazers, have been effective in the widespread restoration of TDFs (Janzen, 2002), but active management interventions are necessary (Holl & Aide, 2011) on degraded soils where succession is arrested.' metadata={'source': 'Using soil amendments and plant functional traits to select native tropical dry forest species for the restoration of degraded Vertisols', 'id': '0.1.0'}\n",
            "page_content='Highly degraded Vertisols, common in NW Costa Rica, are a barrier to the regeneration of TDF on large tracts of land (Gutiérrez, pers.obs.).Vertisols have shrink-swell cycles resulting from high expansive clay content (Deckers, Spaargaren, & Nachtergaele, 2001).Degraded Vertisols shrink and crack during the dry season, and swell during the rainy season, resulting in flooding (Figure 1).Consequently, restoration of these soils is particularly challenging due in part to high seedling mortality rates (Gutiérrez, pers.obs.).To date most studies have focused on developing best practices to use Vertisols for agriculture (Deckers et al., 2001) and not restoration.Most Vertisols in NW Costa Rica were deforested and used for rice cultivation or cattle grazing (Gutiérrez, pers.obs.), and if livestock density is high compaction resulting from grazing can impede restoration (Nepstad, Uhl, & Serrão, 1991).Accordingly, the use of soil amendments during planting may hold promise, as certain amendments facilitate establishment of native TDF seedlings (Fajardo, Rodríguez, González, & Briceño-Linares, 2013).In Vertisols, amendments that ameliorate microclimatic conditions by improving drainage during seasonal flooding may increase initial survivorship and growth when mixed into the rooting zone of seedlings.Other barriers to TDF restoration include selection of species for plantings, as performance data for native tree species are generally unavailable (Butterfield, 1995).' metadata={'source': 'Using soil amendments and plant functional traits to select native tropical dry forest species for the restoration of degraded Vertisols', 'id': '0.1.1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we will generate embeddings using different models for comparison\n",
        "First one is jina-embedding-l-en-v1"
      ],
      "metadata": {
        "id": "hKPYs7QuoLHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/vectorstore\"\n",
        "DB_JINA_EMBEDDING_PATH = 'vectorstore/db_jina-embedding-l-en-v1'\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name='jinaai/jina-embedding-b-en-v1',\n",
        "                                       model_kwargs={'device': 'cuda'})\n",
        "\n",
        "db = FAISS.from_documents(prod_splits, embeddings)\n",
        "serialized_bytes = db.serialize_to_bytes()\n",
        "with open(\"/content/vectorstore/serialized_db.txt\", \"wb\") as binary_file:\n",
        "    # Write bytes to file\n",
        "    binary_file.write(serialized_bytes)\n",
        "\n",
        "db.save_local(DB_JINA_EMBEDDING_PATH)"
      ],
      "metadata": {
        "id": "s2LvMgQOPF4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4a203e-eeb7-4f99-d04d-4f50097cee66"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/vectorstore’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = await db.asimilarity_search(\"I am designing a tropical dry forest restoration in an open field with no remaining tree cover. Should I plant species with higher or lower wood density to maximize initial survival?\")\n",
        "print(len(docs))\n",
        "print(docs[0].metadata)\n",
        "print(docs[1].metadata)\n",
        "print(docs[2].metadata)\n",
        "print(docs[3].metadata)"
      ],
      "metadata": {
        "id": "q-hu1Nn3obvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5902895f-13a4-40d1-f36c-5f21358b8c28"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "{'source': 'Below-ground traits mediate tree survival in a tropical dry forest restoration', 'id': '1.7.0'}\n",
            "{'source': 'Species wood density and the location of planted seedlings drive early‐stage seedling survival during tropical forest restoration', 'id': '6.1.0'}\n",
            "{'source': 'Below-ground traits mediate tree survival in a tropical dry forest restoration', 'id': '1.1.3'}\n",
            "{'source': 'Using large‐scale tropical dry forest restoration to test successional theory', 'id': '11.17.1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "import tiktoken\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('COLABORATIVE_EARTH_KEY')\n",
        "llm_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "  memory_key='chat_history',\n",
        "  return_messages=False\n",
        ")\n",
        "retriever = db.as_retriever(\n",
        "    search_kwargs={\"k\": 20}\n",
        ")\n",
        "\n",
        "custom_template = \"\"\"You are an AI assistant for assisted restoration papers.\n",
        "You are given the following extracted parts of a long document.\n",
        "=========\n",
        "{context}\n",
        "=========\n",
        "Provide a conversational answer for the following question\n",
        "Question: {question}.\n",
        "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
        "Answer in Markdown:\"\"\"\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    template=custom_template,\n",
        "    input_variables=[\"context\", \"question\", \"source\"],\n",
        ")\n",
        "\n",
        "\n",
        "conversation_chain_with_reference_prompt = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        # chain_type=\"stuff\" will go through everything.\n",
        "        # chain_type=\"refine\",\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        verbose=True,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": custom_prompt}\n",
        ")\n",
        "\n",
        "\n",
        "conversation_chain_without_reference = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        # chain_type=\"stuff\" will go through everything.\n",
        "        chain_type=\"refine\",\n",
        "        # chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        # return_source_documents=True,\n",
        "        verbose=True,\n",
        "        memory=memory,\n",
        "        # combine_docs_chain_kwargs={\"question_prompt\": custom_prompt, \"refine_prompt\": custom_prompt}\n",
        ")"
      ],
      "metadata": {
        "id": "3K8kg2VYy3DG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TODO\n",
        "1. Add a cross encoder after the retrieval stage to re-rank the results before feeding it to the API.\n",
        "2. Alternately, or maybe along with this, we may also want to combine several chunks to send as context, depending on the context lenght.\n",
        "3. Or perhaps we also chunk in small portions and combine results to send to OpenAI. This way answers can be formed based on chunks from different papers and different sections.\n",
        "4. Update this RAG to cite sources from the context provided. Refence: https://blog.langchain.dev/langchain-chat/\n",
        "\n",
        "https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a"
      ],
      "metadata": {
        "id": "OLk6TAvQyMip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can plant functional traits be useful when designing a restoration project?\"\n",
        "result = conversation_chain_with_reference_prompt({\"question\": query})\n",
        "# result = conversation_chain_without_reference({\"question\": query})\n",
        "answer = result[\"answer\"]\n",
        "\n",
        "answer"
      ],
      "metadata": {
        "id": "J46fLZZH08dj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}