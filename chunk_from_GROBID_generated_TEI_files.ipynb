{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFXH5UylbzGmT2t6c6Uv0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smthomas1704/restoration-rag/blob/main/chunk_from_GROBID_generated_TEI_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHOBcH3Ir4aD"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==v4.6.3\n",
        "\n",
        "!gdown https://drive.google.com/file/d/1LrXwPMgiok1zcn4i4LND7LOohslSmv8x/view?usp=drive_link -O /content/functional_trait_literature_unsegmented_sentences.zip --fuzzy\n",
        "\n",
        "!unzip /content/functional_trait_literature_unsegmented_sentences.zip\n",
        "\n",
        "!pip install beautifulsoup4\n",
        "!pip install lxml\n",
        "!pip install pandas\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing TEI files.\n",
        "1. We will be splitting the GROBID generated TEI files into smaller chunks.\n",
        "2. Only paragraphs with titles and body will be used. Will skip references at the end of the page.\n",
        "3. However, we do want to include the inline references to other material. Need to figure out how to do that.\n",
        "\n",
        "### References:\n",
        "1. https://kermitt2-grobid.hf.space/\n",
        "2. https://python.langchain.com/docs/integrations/document_loaders/grobid\n",
        "3. https://research.google.com/colaboratory/local-runtimes.html#:~:text=You%20can%20either%20run%20Jupyter,and%20the%20resource%20utilization%20monitor.\n",
        "4. https://pypi.org/project/grobid-tei-xml/\n",
        "5. https://stackoverflow.com/questions/2136267/beautiful-soup-and-extracting-a-div-and-its-contents-by-id\n",
        "6. https://grobid.readthedocs.io/en/latest/Grobid-docker/\n",
        "\n",
        "\n",
        "### TODO\n",
        "1. Also get \"figDesc\" to the chunks. Some of these figure descriptions have a lot of information.\n",
        "2. Possibly we should also eliminate some useless chunks such as acknowledgements, conflict of interest statements etc"
      ],
      "metadata": {
        "id": "f_SBeos7nX2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "tei_file_list = os.listdir(\"/content/tei_all_afr_carbon_2\")\n",
        "chunks = []\n",
        "large_chunks = []\n",
        "abstracts_only = []\n",
        "for i, f in enumerate(tei_file_list):\n",
        "  print(f)\n",
        "  with open(f\"/content/tei_all_afr_carbon_2/{f}\", 'r') as tei:\n",
        "      soup = BeautifulSoup(tei, 'xml')\n",
        "      sections = soup.find_all(\"div\")\n",
        "      heads = soup.find_all(\"head\")\n",
        "      title = soup.find_all(\"title\")[0].text\n",
        "      keywords = []\n",
        "\n",
        "      for keyword in soup.find_all(\"keywords\"):\n",
        "        for term in keyword.find_all(\"term\"):\n",
        "          keywords.append(term.get_text())\n",
        "\n",
        "      abstract = soup.find(\"abstract\").get_text()\n",
        "      abstracts_only.append(abstract)\n",
        "      abstracts_only.append({\n",
        "          \"file_name\": f,\n",
        "          \"page_content\": abstract,\n",
        "          \"title\": title,\n",
        "          \"id\": f\"{i}\",\n",
        "          \"keywords\": keywords\n",
        "      })\n",
        "\n",
        "      for j, section in enumerate(sections):\n",
        "          head = section.find_all(\"head\")\n",
        "          # Only consider the paragraphs that have a head\n",
        "          combined_paras = []\n",
        "          if len(head) > 0:\n",
        "              # large_chunks.append(section.get_text())\n",
        "              paragraphs = section.findAll(\"p\")\n",
        "              # Each paragraph can be a chunk. When feeding context we will get several similar\n",
        "              # chunks and combine it to pass context\n",
        "              for k, para in enumerate(paragraphs):\n",
        "                obj = {\n",
        "                    \"file_name\": f,\n",
        "                    \"page_content\": para.get_text(),\n",
        "                    \"title\": title,\n",
        "                    \"id\": f\"{i}.{j}.{k}\",\n",
        "                    \"keywords\": keywords\n",
        "                }\n",
        "                combined_paras.append(para.get_text())\n",
        "          if len(combined_paras) > 0:\n",
        "            chunks.extend(combined_paras)\n",
        "            large_chunk = \"\\n\".join(combined_paras)\n",
        "            # print(large_chunk)\n",
        "            large_chunks.append({\n",
        "                \"file_name\": f,\n",
        "                \"page_content\": large_chunk,\n",
        "                \"title\": title,\n",
        "                \"id\": f\"{i}.{j}\",\n",
        "                \"keywords\": keywords\n",
        "            })\n",
        "\n",
        "\n",
        "# Write chunks to jsonl file.\n",
        "with open(\"/content/all_afr_carbon_small_chunks.jsonl\", \"w\") as final:\n",
        "  json.dump(chunks, final, indent=2)\n",
        "\n",
        "with open(\"/content/all_afr_carbon_large_chunks.jsonl\", \"w\") as final:\n",
        "  json.dump(large_chunks, final, indent=2)\n",
        "\n",
        "with open(\"/content/all_afr_carbon_abstracts_only.jsonl\", \"w\") as final:\n",
        "  json.dump(abstracts_only, final, indent=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "WF8GwpRuptCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploaded the file to Huggingface. Now download it in the next section\n",
        "\n",
        "TODO:\n",
        "1. Programmatically upload dataset to Huggingface"
      ],
      "metadata": {
        "id": "rmFFbCFbU2LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Z0Rdks6DoXzW",
        "outputId": "de20ef62-7fd2-443f-94bd-e90160dfa981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import hf_hub_download\n",
        "import pandas as pd\n",
        "\n",
        "HUGGINGFACE_TOKEN = userdata.get(\"HUGGINGFACE_TOKEN\")\n",
        "REPO_ID = \"collaborativeearth/functional_trait_papers\"\n",
        "FILENAME = \"all_afr_carbon_small_chunks.jsonl\"\n",
        "\n",
        "# Currently the dataset is not gated, thats why we're able to download it like\n",
        "dataset = load_dataset(REPO_ID)\n",
        "\n",
        "dataset.push_to_hub(FILENAME, token=HUGGINGFACE_TOKEN)\n",
        "\n",
        "# print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "cFRT8MZvU1zM",
        "outputId": "253533cf-8785-4e64-d345-6273b4b78b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Directory all_afr_carbon_small_chunks.jsonl is neither a `Dataset` directory nor a `DatasetDict` directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4daf4a9d203f>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Currently the dataset is not gated, thats why we're able to download it like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREPO_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHUGGINGFACE_TOKEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m         raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   2672\u001b[0m             \u001b[0;34mf\"Directory {dataset_path} is neither a `Dataset` directory nor a `DatasetDict` directory.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m         )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Directory all_afr_carbon_small_chunks.jsonl is neither a `Dataset` directory nor a `DatasetDict` directory."
          ]
        }
      ]
    }
  ]
}