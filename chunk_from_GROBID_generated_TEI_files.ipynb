{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMj0nvBptuDsAYP4K1q60Pc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smthomas1704/restoration-rag/blob/main/chunk_from_GROBID_generated_TEI_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qHOBcH3Ir4aD",
        "outputId": "34ea256e-8a79-45f2-ae86-63ac3b7b00c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.3 langchain-community-0.0.15 langchain-core-0.1.15 langsmith-0.0.83 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown==v4.6.3\n",
        "\n",
        "!gdown https://drive.google.com/file/d/1U8mrDhhHdrLxQGn5TEIlemMV-46f9oLL/view?usp=drive_link -O /content/functional_trait_literature_unsegmented_sentences.zip --fuzzy\n",
        "\n",
        "!unzip /content/functional_trait_literature_unsegmented_sentences.zip\n",
        "\n",
        "!pip install beautifulsoup4\n",
        "!pip install lxml\n",
        "!pip install pandas\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing TEI files.\n",
        "1. We will be splitting the GROBID generated TEI files into smaller chunks.\n",
        "2. Only paragraphs with titles and body will be used. Will skip references at the end of the page.\n",
        "3. However, we do want to include the inline references to other material. Need to figure out how to do that.\n",
        "\n",
        "### References:\n",
        "1. https://kermitt2-grobid.hf.space/\n",
        "2. https://python.langchain.com/docs/integrations/document_loaders/grobid\n",
        "3. https://research.google.com/colaboratory/local-runtimes.html#:~:text=You%20can%20either%20run%20Jupyter,and%20the%20resource%20utilization%20monitor.\n",
        "4. https://pypi.org/project/grobid-tei-xml/\n",
        "5. https://stackoverflow.com/questions/2136267/beautiful-soup-and-extracting-a-div-and-its-contents-by-id\n",
        "6. https://grobid.readthedocs.io/en/latest/Grobid-docker/"
      ],
      "metadata": {
        "id": "f_SBeos7nX2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "tei_file_list = os.listdir(\"/content/funtional_trait_literature_grobid_tei\")\n",
        "chunks = []\n",
        "for i, f in enumerate(tei_file_list):\n",
        "  print(f)\n",
        "  with open(f\"/content/funtional_trait_literature_grobid_tei/{f}\", 'r') as tei:\n",
        "      soup = BeautifulSoup(tei, 'xml')\n",
        "      sections = soup.find_all(\"div\")\n",
        "      heads = soup.find_all(\"head\")\n",
        "      title = soup.find_all(\"title\")[0].text\n",
        "      print(f\"Title: {title}\")\n",
        "      print(\"Abstract: \")\n",
        "      print(soup.find(\"abstract\").get_text())\n",
        "      keywords = []\n",
        "\n",
        "      for keyword in soup.find_all(\"keywords\"):\n",
        "        for term in keyword.find_all(\"term\"):\n",
        "          keywords.append(term.get_text())\n",
        "\n",
        "      for j, section in enumerate(sections):\n",
        "          head = section.find_all(\"head\")\n",
        "          # Only consider the paragraphs that have a head\n",
        "          if len(head) > 0:\n",
        "              paragraphs = section.findAll(\"p\")\n",
        "              # Each paragraph can be a chunk. When feeding context we will get several similar\n",
        "              # chunks and combine it to pass context\n",
        "              for k, para in enumerate(paragraphs):\n",
        "                obj = {\n",
        "                    \"file_name\": f,\n",
        "                    \"page_content\": para.get_text(),\n",
        "                    \"title\": title,\n",
        "                    \"id\": f\"{i}.{j}.{k}\",\n",
        "                    \"keywords\": keywords\n",
        "                }\n",
        "                print(obj)\n",
        "                chunks.append(obj)\n",
        "\n",
        "\n",
        "# Write chunks to jsonl file.\n",
        "with open(\"/content/function_trait_paper_small_chunks.jsonl\", \"w\") as final:\n",
        "  json.dump(chunks, final, indent=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "WF8GwpRuptCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploaded the file to Huggingface. Now download it in the next section\n",
        "\n",
        "TODO:\n",
        "1. Programmatically upload dataset to Huggingface"
      ],
      "metadata": {
        "id": "rmFFbCFbU2LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import pandas as pd\n",
        "\n",
        "REPO_ID = \"collaborativeearth/functional_trait_papers\"\n",
        "FILENAME = \"function_trait_paper_small_chunks.jsonl\"\n",
        "\n",
        "# Currently the dataset is not gated, thats why we're able to download it like this\n",
        "# Otherwise authentication is required\n",
        "dataset = pd.read_json(\n",
        "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")\n",
        ")\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "cFRT8MZvU1zM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}