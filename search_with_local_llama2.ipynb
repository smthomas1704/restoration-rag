{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smthomas1704/restoration-rag/blob/main/search_with_local_llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK_xJuRHp3pS"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/smthomas1704/restoration-rag.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdwhiRurqDT6"
      },
      "outputs": [],
      "source": [
        "!pip install -r restoration-rag/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BGc8SNPqTPY"
      },
      "outputs": [],
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-7B-chat-GGML\"\n",
        "model_basename = \"llama-2-7b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "xpvvXw6Bnwcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from llama_cpp import Llama\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# for token-wise streaming so you'll see the answer gets generated token by token when Llama is answering your question\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path, # https://huggingface.co/TheBloke/Llama-2-7B-GGUF\n",
        "    temperature=0.7, # for factual answers\n",
        "    top_p=0.1,\n",
        "    n_ctx=6000,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# lcpp_llm = Llama(\n",
        "#     model_path=model_path,\n",
        "#     n_threads=2, # CPU cores\n",
        "#     n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "#     n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "#   )\n",
        "\n",
        "# See the number of layers in GPU\n",
        "# print(lcpp_llm.params.n_gpu_layers)\n",
        "\n",
        "\n",
        "# question = \"who wrote the book Innovator's dilemma?\"\n",
        "# This runs very very slow as it tries to hallucinate every single character one by one\n",
        "# Last time it took more than 10mins.\n",
        "# answer = llm(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ865rkukJwB",
        "outputId": "8d134ee0-ceb6-4691-cb3c-1eb16eae9090"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next section is about processing the pdf files that we have into smaller chunks and generating embeddings for them. The embeddings will be stored in a vector DB and queried by LangChain.\n",
        "\n",
        "Let's start with downloading, splitting and storing the files"
      ],
      "metadata": {
        "id": "hZjEGExlr8UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "DATA_PATH = 'restoration-rag/data' #Your root data folder path\n",
        "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
        "\n",
        "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
        "documents = loader.load()\n",
        "\n",
        "print(len(documents))\n",
        "print(documents[0].page_content[0:100])\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(splits[31])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsUx4gfUqlUZ",
        "outputId": "fc372d3a-163e-4d0b-c8ba-d9301b142972"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38\n",
            "Ecol Solut Evid.  2023;4:e12246.\t \t\t \t | 1 of 11\n",
            "https://doi.org/10.1002/2688-8319.12246\n",
            "wileyonline\n",
            "page_content='production model of knowledge production assumes reciprocal knowledge flow between science and practice; conservation practitioners, \\nscientists and other stakeholders (i.e. the people invested in and affected by conservation decisions) jointly create actionable knowledge \\nby working together to define research needs, set research agendas, implement research and generate products (e.g. data, publications,' metadata={'source': 'restoration-rag/data/20230400982.pdf', 'page': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': 'cpu'})\n",
        "\n",
        "db = FAISS.from_documents(splits, embeddings)\n",
        "db.save_local(DB_FAISS_PATH)\n",
        "\n",
        "\n",
        "# If we ever need to load this back up from the files, this is the code\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "#                                        model_kwargs={'device': 'cpu'})\n",
        "# db = FAISS.load_local(DB_FAISS_PATH, embeddings)"
      ],
      "metadata": {
        "id": "gmIFNIJLtkNb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from queue import Queue\n",
        "from typing import Any\n",
        "from langchain.llms.huggingface_text_gen_inference import HuggingFaceTextGenInference\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.schema import LLMResult\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from anyio.from_thread import start_blocking_portal #For model callback streaming\n",
        "\n",
        "langchain.debug=True\n",
        "\n",
        "template = \"\"\"\n",
        "[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n",
        "{context}\n",
        "Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "retriever = db.as_retriever(\n",
        "        search_kwargs={\"k\": 6}\n",
        "    )\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": PromptTemplate(\n",
        "            template=template,\n",
        "            input_variables=[\"context\", \"question\"],\n",
        "        ),\n",
        "    }\n",
        ")\n",
        "\n",
        "result = qa_chain({\"query\": \"How to prioritize areas for ecological restoration\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3igHNKxufZw",
        "outputId": "8a85363a-2483-46b9-9385-1608cc5e6f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"query\": \"How to prioritize areas for ecological restoration\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"How to prioritize areas for ecological restoration\",\n",
            "  \"context\": \"Climent-  Gil, E., Derak, M., López, G., Bonet, A., Aledo, A., & \\nCortina-  Segarra, J. (2023). Prioritizing areas for ecological \\nrestoration: A participatory approach based on cost-  \\neffectiveness. Journal of Applied Ecology , 60, 1194–1205. \\nhttps://doi.org/10.1111/1365-2664.14395\\n\\nLandscape-  scale prioritization models reflect alternative ap -\\nproaches to assessing the effectiveness of restoration actions. \\nThese models have used multiple criteria to define priority areas for \\nrestoration, including disaster risk reduction (Vogler et al., 2015 ), \\npast and future species distribution (Yoshioka et al.,  2014 ), vulnera -\\nble ecosystems (Etter et al., 2020 ), multiple ES (Comín et al., 2018 ), \\nlandscape connectivity (García-  Feced et al., 2011 ) and socioeco -\\n\\n1194  |     J Appl Ecol. 2023;60:1194–1205.\\n wileyonlinelibrary.com/journal/jpeReceived: 1 April 2021  | Accepted: 4 March 2023\\nDOI: 10.1111/1365-2664.14395  \\nRESEARCH ARTICLE\\nPrioritizing areas for ecological restoration: A participatory \\napproach based on cost-  effectiveness\\nElysa Silva1 |   Walid  Naji1 |   Pietro Salvaneschi2 |   Emilio  Climent-  Gil1 |   \\nMchich  Derak3 |   Germán López1 |   Andreu Bonet1 |   Antonio Aledo1 |   \\nJordi  Cortina-  Segarra1\\n\\n4. Synthesis and applications . The cost-  effectiveness of restoration actions depends \\non the type of ecosystem and degradation state. Visualizing the outcomes of  \\nalternative restoration scenarios needs participatory prioritization maps based on \\nfinancial costs and the potential supply of ecosystem services. We propose a par -\\nticipatory prioritization protocol that is flexible and adaptable and can help gov -\\nernment agencies, environmental managers, investors, consultancies and NGOs'\\n\\n|  1203 Journal of Applied Ecology\\nSILVA  et al.\\nshould help quantify ecosystem restorability, identify restoration \\nthresholds, and integrate climatic and land-  use changes, thus en -\\nriching our assumptions on ecosystem dynamics and their impact \\non the supply of ES.\\n5 | CONCLUSIONS\\nWe show here that a prioritization strategy based on participatory \\nCEA of ecological restoration actions can maximize the outcomes \\nof investments while responding to societal preferences and needs.\\n\\n75% LOW 14 72\\n75% RAN 30 62\\n75% HIGH 64 60\\n50% LOW 9 48\\n50% RAN 20 41\\n50% HIGH 42 40\\n25% LOW 5 24\\n25% RAN 10 21\\n25% HIGH 21 20\\nFIGURE 5 Total gain of ecosystem services Sections under 12 scenarios of prioritization. These are combinations of three strategies to \\nrestore 15% of the area considering the degradation state (LOW for less degraded areas, HIGH for highly degraded areas, and RAN for a\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:LlamaCpp] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\\nCliment-  Gil, E., Derak, M., López, G., Bonet, A., Aledo, A., & \\nCortina-  Segarra, J. (2023). Prioritizing areas for ecological \\nrestoration: A participatory approach based on cost-  \\neffectiveness. Journal of Applied Ecology , 60, 1194–1205. \\nhttps://doi.org/10.1111/1365-2664.14395\\n\\nLandscape-  scale prioritization models reflect alternative ap -\\nproaches to assessing the effectiveness of restoration actions. \\nThese models have used multiple criteria to define priority areas for \\nrestoration, including disaster risk reduction (Vogler et al., 2015 ), \\npast and future species distribution (Yoshioka et al.,  2014 ), vulnera -\\nble ecosystems (Etter et al., 2020 ), multiple ES (Comín et al., 2018 ), \\nlandscape connectivity (García-  Feced et al., 2011 ) and socioeco -\\n\\n1194  |     J Appl Ecol. 2023;60:1194–1205.\\n wileyonlinelibrary.com/journal/jpeReceived: 1 April 2021  | Accepted: 4 March 2023\\nDOI: 10.1111/1365-2664.14395  \\nRESEARCH ARTICLE\\nPrioritizing areas for ecological restoration: A participatory \\napproach based on cost-  effectiveness\\nElysa Silva1 |   Walid  Naji1 |   Pietro Salvaneschi2 |   Emilio  Climent-  Gil1 |   \\nMchich  Derak3 |   Germán López1 |   Andreu Bonet1 |   Antonio Aledo1 |   \\nJordi  Cortina-  Segarra1\\n\\n4. Synthesis and applications . The cost-  effectiveness of restoration actions depends \\non the type of ecosystem and degradation state. Visualizing the outcomes of  \\nalternative restoration scenarios needs participatory prioritization maps based on \\nfinancial costs and the potential supply of ecosystem services. We propose a par -\\nticipatory prioritization protocol that is flexible and adaptable and can help gov -\\nernment agencies, environmental managers, investors, consultancies and NGOs'\\n\\n|  1203 Journal of Applied Ecology\\nSILVA  et al.\\nshould help quantify ecosystem restorability, identify restoration \\nthresholds, and integrate climatic and land-  use changes, thus en -\\nriching our assumptions on ecosystem dynamics and their impact \\non the supply of ES.\\n5 | CONCLUSIONS\\nWe show here that a prioritization strategy based on participatory \\nCEA of ecological restoration actions can maximize the outcomes \\nof investments while responding to societal preferences and needs.\\n\\n75% LOW 14 72\\n75% RAN 30 62\\n75% HIGH 64 60\\n50% LOW 9 48\\n50% RAN 20 41\\n50% HIGH 42 40\\n25% LOW 5 24\\n25% RAN 10 21\\n25% HIGH 21 20\\nFIGURE 5 Total gain of ecosystem services Sections under 12 scenarios of prioritization. These are combinations of three strategies to \\nrestore 15% of the area considering the degradation state (LOW for less degraded areas, HIGH for highly degraded areas, and RAN for a\\nQuestion: How to prioritize areas for ecological restoration [/INST]\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DF9kEBW3uja5"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjVXFipq8HUrtvqzIiMdUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}