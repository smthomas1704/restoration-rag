{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIjpNsK9P8cMOg2ZaRZMJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smthomas1704/restoration-rag/blob/main/generate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download\n",
        "Download the zipped literature file from this Google Drive location"
      ],
      "metadata": {
        "id": "-PnazWDSRIPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0MbHH-sKD9i",
        "outputId": "14f9c6d7-a463-4160-e0df-b96b411de16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10_inKhFuY5O8Sel88ZvlTqODsbbh4ula\n",
            "To: /content/functional_trait_literature_zipped.zip\n",
            "\r  0% 0.00/9.80M [00:00<?, ?B/s]\r 11% 1.05M/9.80M [00:00<00:01, 8.34MB/s]\r100% 9.80M/9.80M [00:00<00:00, 46.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/file/d/10_inKhFuY5O8Sel88ZvlTqODsbbh4ula/view?usp=drive_link -O /content/functional_trait_literature_zipped.zip --fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/functional_trait_literature_zipped.zip"
      ],
      "metadata": {
        "id": "6PJWUscXKlE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk and store\n",
        "\n",
        "In this portion, we'll chunk all the files into smaller paragraphs and use that for generating embeddings. We'll separately store the chunks so we can use it later, without having to re-download all the literature again.\n",
        "\n",
        "### References/Notes related to chunking\n",
        "1. https://openai.com/blog/new-and-improved-embedding-model\n",
        "2. text-embedding-ada-002 is the best model for text embedding generation\n",
        "3. https://www.pinecone.io/learn/chunking-strategies/\n",
        "4. https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pypdf\n",
        "\n",
        "### Possible strategies for chunking\n",
        "1. LaTex: LaTeX is a document preparation system and markup language often used for academic papers and technical documents. By parsing the LaTeX commands and environments, you can create chunks that respect the logical organization of the content (e.g., sections, subsections, and equations), leading to more accurate and contextually relevant results.\n",
        "2. Latex taxes a string as input, so we will need to read\n",
        "3. Most of these academic papers are written in Latex and then converted to PDF. Our best bet would be to convert the PDF to Latex format first and then use the Latex based chunker to chunk things. This way paragraphs and related information will be together and contextualized.\n",
        "4. On the other hand its not a guarantee that the document was first written in LaTex."
      ],
      "metadata": {
        "id": "aahoQgRnKfZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.text_splitter import LatexTextSplitter\n",
        "# latex_text = \"...\"\n",
        "# latex_splitter = LatexTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "# docs = latex_splitter.create_documents([latex_text])"
      ],
      "metadata": {
        "id": "K5AJyyJUQo_1"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}