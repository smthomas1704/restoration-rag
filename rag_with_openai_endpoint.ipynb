{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSvbJFm2SPhSN+Vpq6TbPp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smthomas1704/restoration-rag/blob/main/rag_with_openai_endpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HytFcB6aphPW",
        "outputId": "a5acaca4-3897-4bc9-8fa5-052dd61ce398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'restoration-rag' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Reference: https://colab.research.google.com/drive/1uRyGh-0wpyAveJAQoyK4YXiFfOyAkLAx?usp=sharing#scrollTo=wb30MdMQBJ7p\n",
        "\n",
        "!git clone https://github.com/smthomas1704/restoration-rag.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-gpu\n",
        "!pip install langchain_experimental\n",
        "!pip install \"langchain[docarray]\""
      ],
      "metadata": {
        "id": "EmLByQThqspn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8c82fc-aa9e-48c3-e732-caa0af7bb858"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287621 sha256=85b12decc3e5856c472d46ee39ae8baad495b62815334c3aaf0899d372fdb2d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: types-requests, orjson, hnswlib, docarray\n",
            "Successfully installed docarray-0.32.1 hnswlib-0.8.0 orjson-3.9.10 types-requests-2.31.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('COLABORATIVE_EARTH_KEY')\n",
        "llm_model = \"gpt-3.5-turbo\""
      ],
      "metadata": {
        "id": "hb4dydMMqDDF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import tiktoken\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "4Jklk8Yfqlst"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "  ],\n",
        "  max_tokens=10\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "weqct7r8rNzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8500866-0037-4122-c00c-5dfd7fb5786d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8XcXhejcl2OAjvzGIveNOS1t63ED4', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played in Arlington', role='assistant', function_call=None, tool_calls=None))], created=1703023673, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=53, total_tokens=63))\n"
          ]
        }
      ]
    }
  ]
}